# GitLab CI E2E Tests Configuration
# This file contains e2e test jobs using Sysbox runners with kind clusters
#
# Sysbox runners allow running Docker-in-Docker securely which is required
# for kind (Kubernetes in Docker) clusters.
#
# Available runner tags:
#   aws_run_sysbox / aws_run_sysbox_s: 2 CPUs, 4 GB RAM
#   aws_run_sysbox_m: 4 CPUs, 8 GB RAM
#   aws_run_sysbox_l: 8 CPUs, 16 GB RAM
#   aws_run_sysbox_xl: 16 CPUs, 64 GB RAM

variables:
  # E2E test configuration
  E2E_IMG: "auth-operator:e2e-test"
  KIND_VERSION: "v0.31.0"
  KIND_NODE_IMAGE: "kindest/node:v1.34.3"
  HELM_VERSION: "v3.17.0"
  # Go download version - must be exact version for go.dev downloads
  GO_DOWNLOAD_VERSION: "1.25.6"
  # Docker configuration for sysbox DinD
  DOCKER_DRIVER: overlay2
  DOCKER_HOST: tcp://docker:2375
  DOCKER_TLS_CERTDIR: ""
  # Go configuration
  GOPROXY: "https://proxy.golang.org,direct"
  GOFLAGS: "-mod=readonly"

# Base template for e2e jobs with sysbox runner
.e2e-base:
  stage: test
  tags:
    - aws_run_sysbox_m
  services:
    - name: 'dockerhub.devops.telekom.de/docker:25.0.3-dind'
      command:
        - '--tls=false'
        - '--host=tcp://0.0.0.0:2375'
        - '--registry-mirror=https://dockerhub.devops.telekom.de'
        # Storage driver optimizations for nested containers
        - '--storage-driver=overlay2'
        # Use systemd cgroup driver for consistency with kind nodes
        - '--exec-opt=native.cgroupdriver=systemd'
        # Increase default shm size for kind nodes
        - '--default-shm-size=256M'
      alias: docker
  image: dockerhub.devops.telekom.de/docker:25.0.3
  before_script:
    - |
      set -exo pipefail
      echo "=== Setting up e2e test environment ==="

      # Wait for Docker daemon to be ready with extended timeout
      echo "Waiting for Docker daemon..."
      timeout 120 sh -c 'until docker info >/dev/null 2>&1; do sleep 2; done'
      docker info

      # Install required tools
      echo "Installing tools..."
      apk add --no-cache curl bash git make gcc musl-dev

      # Install Go (Alpine's default is too old, we need Go ${GO_DOWNLOAD_VERSION})
      echo "Installing Go ${GO_DOWNLOAD_VERSION}..."
      GO_DOWNLOAD_URL="https://go.dev/dl/go${GO_DOWNLOAD_VERSION}.linux-amd64.tar.gz"
      echo "Downloading from: ${GO_DOWNLOAD_URL}"
      curl -fsSL "${GO_DOWNLOAD_URL}" -o /tmp/go.tar.gz
      tar -C /usr/local -xzf /tmp/go.tar.gz
      rm /tmp/go.tar.gz
      export PATH="/usr/local/go/bin:$PATH"
      export GOPATH="/go"
      export PATH="$GOPATH/bin:$PATH"
      go version

      # Install kubectl
      echo "Installing kubectl..."
      curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
      chmod +x kubectl && mv kubectl /usr/local/bin/
      kubectl version --client

      # Install kind
      echo "Installing kind ${KIND_VERSION}..."
      curl -Lo ./kind "https://kind.sigs.k8s.io/dl/${KIND_VERSION}/kind-linux-amd64"
      chmod +x ./kind && mv ./kind /usr/local/bin/kind
      kind version

      # Install Helm
      echo "Installing Helm ${HELM_VERSION}..."
      curl -fsSL https://get.helm.sh/helm-${HELM_VERSION}-linux-amd64.tar.gz | tar xzf - -C /tmp
      mv /tmp/linux-amd64/helm /usr/local/bin/helm
      helm version

      # Verify Go environment is set up correctly
      echo "Go environment:"
      go version
      go env GOPATH GOROOT

      # Check if seccomp is supported in this environment
      # This is critical for kind clusters - seccomp failures prevent pod creation
      check_seccomp_support() {
        echo "Checking seccomp support..."
        local seccomp_status
        seccomp_status=$(grep -E '^Seccomp:' /proc/self/status 2>/dev/null | awk '{print $2}' || echo "unknown")
        echo "Seccomp status: $seccomp_status"
        if [ "$seccomp_status" = "0" ]; then
          echo "WARNING: Seccomp is DISABLED in this environment!"
          echo "This typically happens when:"
          echo "  - Running amd64 containers on ARM via QEMU emulation"
          echo "  - Running in certain restricted container runtimes"
          echo "  - Kernel seccomp support is disabled"
          echo "Kind clusters may fail to start pods without seccomp support."
          return 1
        elif [ "$seccomp_status" = "unknown" ]; then
          echo "WARNING: Could not determine seccomp status"
          echo "Proceeding with default configuration..."
          return 0
        else
          echo "Seccomp is enabled (mode: $seccomp_status)"
          return 0
        fi
      }

      # Prepare kind config - adds containerd patches if seccomp is not supported
      prepare_kind_config() {
        local config_template=$1
        local output_config=$2
        echo "Preparing kind config from $config_template..."
        if check_seccomp_support; then
          cp "$config_template" "$output_config"
        else
          echo "Adding containerd patches to disable seccomp..."
          cp "$config_template" "$output_config"
          echo 'containerdConfigPatches:' >> "$output_config"
          echo '  - |-' >> "$output_config"
          echo '    [plugins."io.containerd.grpc.v1.cri"]' >> "$output_config"
          echo '      disable_seccomp = true' >> "$output_config"
        fi
        echo "Prepared kind config:"
        cat "$output_config"
      }

      # Define helper function for kind cluster creation with retry
      create_kind_cluster() {
        local name=$1
        local config=$2
        local wait_time=${3:-5m}
        local max_retries=2
        local retry=0

        # Prepare config with seccomp handling
        local prepared_config="/tmp/kind-config-prepared.yaml"
        prepare_kind_config "$config" "$prepared_config"

        # Dump network info before cluster creation
        echo "=== Pre-cluster network diagnostics ==="
        echo "Docker networks:"
        docker network ls 2>&1 || true
        echo "Current container's network (if any):"
        cat /etc/hosts 2>&1 || true
        echo "Hostname resolution test:"
        getent hosts docker 2>&1 || echo "Could not resolve 'docker' hostname"
        ping -c 1 docker 2>&1 || echo "Could not ping 'docker'"

        while [ $retry -lt $max_retries ]; do
          echo "Attempting to create kind cluster (attempt $((retry + 1))/$max_retries)..."
          echo "  Cluster name: $name"
          echo "  Config: $prepared_config (from $config)"
          echo "  Wait time: $wait_time"
          echo "  Kind image: ${KIND_NODE_IMAGE}"

          # Clean up any existing cluster
          kind delete cluster --name "$name" 2>/dev/null || true

          # Determine network strategy:
          # In GitLab CI with DinD, kind creates its own "kind" network inside the DinD container.
          # This network is isolated from the job container's network.
          # We can try to use the host network mode or bridge network to improve connectivity.

          # Check if we should try using host network (DinD shares network with host)
          # Note: This may not work in all DinD setups
          local network_flag=""
          if [ "$retry" -eq 0 ]; then
            echo "First attempt: using default kind network"
            network_flag=""
          else
            echo "Retry attempt: trying with host network to bypass network isolation"
            # On retry, try using docker's default network mode
            # This helps when kind's network is isolated from the job container
            network_flag=""
          fi

          if kind create cluster \
            --name "$name" \
            --config "$prepared_config" \
            --image "${KIND_NODE_IMAGE}" \
            --wait "$wait_time" \
            --verbosity 6 \
            --retain; then
            echo "Kind cluster created successfully!"

            # Fix kubeconfig for DinD - try multiple connectivity methods
            if fix_kubeconfig_for_dind "$name"; then
              return 0
            else
              echo "WARNING: Cluster created but API server not accessible!"
              echo "Attempting alternate network configuration..."

              # Try to attach the kind control plane to the bridge network
              echo "Attaching kind control-plane to bridge network..."
              docker network connect bridge "${name}-control-plane" 2>&1 || echo "Could not connect to bridge network"

              # Get the new IP on bridge network
              echo "New container network settings:"
              docker inspect "${name}-control-plane" --format='{{json .NetworkSettings.Networks}}' 2>&1 | head -20 || true

              # Try fixing kubeconfig again
              if fix_kubeconfig_for_dind "$name"; then
                return 0
              fi

              echo "Still cannot access API server after network reconfiguration"
            fi
          else
            # kind create cluster failed - gather debug info about the failure
            echo "=== Kind cluster creation failed ==="
            echo "=== Gathering worker node join debug info ==="

            # Check all node containers
            echo "=== All kind node containers ==="
            docker ps -a --filter "name=${name}" 2>&1 || true

            # Get logs from all nodes
            for node in $(docker ps -a --filter "name=${name}" --format '{{.Names}}' 2>/dev/null); do
              echo "=== Logs from $node ==="
              docker logs "$node" 2>&1 | tail -100 || true

              echo "=== Kubelet status in $node ==="
              docker exec "$node" systemctl status kubelet 2>&1 | head -30 || true

              echo "=== Kubelet journal in $node ==="
              docker exec "$node" journalctl -u kubelet --no-pager -n 50 2>&1 || true
            done

            # Export kind logs
            echo "=== Exporting kind logs ==="
            kind export logs "test/e2e/output/kind-logs-join-failure" --name "$name" 2>&1 || true
          fi

          # Check for seccomp-related failure
          echo "=== Checking for seccomp-related failures ==="
          if docker logs "${name}-control-plane" 2>&1 | grep -q "seccomp is not supported"; then
            echo "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
            echo "!!! FATAL: Seccomp is not supported in this environment             !!!"
            echo "!!! This is a known issue with QEMU user-space emulation           !!!"
            echo "!!! (e.g., running amd64 containers on ARM hosts)                   !!!"
            echo "!!! Possible solutions:                                              !!!"
            echo "!!!   1. Use native architecture (arm64 on ARM, amd64 on x86)       !!!"
            echo "!!!   2. Use a VM-based virtualization instead of QEMU emulation    !!!"
            echo "!!!   3. Use sysbox runtime which provides proper seccomp support   !!!"
            echo "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
            kind export logs "test/e2e/output/kind-logs-seccomp-failure" --name "$name" 2>/dev/null || true
            return 2
          fi

          retry=$((retry + 1))
          if [ $retry -lt $max_retries ]; then
            echo "=== Cluster creation failed, collecting diagnostics before retry ==="

            echo "=== Docker container status ==="
            docker ps -a 2>&1 || true

            echo "=== Kind node container logs ==="
            docker logs "${name}-control-plane" 2>&1 | tail -50 || true

            echo "=== Kind export logs ==="
            kind export logs "test/e2e/output/kind-logs-attempt-$retry" --name "$name" 2>/dev/null || true

            kind delete cluster --name "$name" 2>/dev/null || true
            sleep 5
          fi
        done

        echo "=== FATAL: Failed to create kind cluster after $max_retries attempts ==="
        echo "=== Collecting final failure diagnostics ==="

        # Check for common failure causes
        echo "=== Checking for known failure patterns ==="
        echo "Seccomp status in this environment:"
        grep -E '^Seccomp' /proc/self/status 2>/dev/null || echo "  Could not read seccomp status"
        if docker logs "${name}-control-plane" 2>&1 | grep -q "seccomp is not supported"; then
          echo "*** DETECTED: Seccomp failure in container logs ***"
          echo "The kubelet cannot create pod sandboxes because seccomp is not supported."
          echo "This is typically caused by running amd64 containers on ARM via QEMU emulation."
        fi

        echo "=== Docker container status ==="
        docker ps -a 2>&1 || true

        echo "=== Docker images ==="
        docker images 2>&1 || true

        echo "=== Kind node container logs ==="
        docker logs "${name}-control-plane" 2>&1 | tail -100 || true

        echo "=== Kind node kubelet logs (checking for seccomp errors) ==="
        docker exec "${name}-control-plane" journalctl -u kubelet --no-pager 2>&1 | grep -E "seccomp|sandbox|error" | tail -30 || true

        echo "=== Kind node container inspect ==="
        docker inspect "${name}-control-plane" 2>&1 | head -150 || true

        echo "=== Docker network list ==="
        docker network ls 2>&1 || true
        docker network inspect kind 2>&1 | head -100 || true

        echo "=== System resources ==="
        free -m 2>&1 || cat /proc/meminfo || true
        df -h 2>&1 || true

        echo "=== Kind export logs ==="
        kind export logs "test/e2e/output/kind-logs-final" --name "$name" 2>/dev/null || true
        find "test/e2e/output/kind-logs-final" -type f -name "*.log" -exec echo "=== {} ===" \; -exec tail -30 {} \; 2>/dev/null || true

        return 1
      }

      # Helper function to fix kubeconfig for DinD environments
      # In GitLab CI with DinD service, the job container and DinD are on different networks.
      # This function tries multiple connectivity methods and provides extensive debugging.
      fix_kubeconfig_for_dind() {
        local cluster_name=$1
        local control_plane_container="${cluster_name}-control-plane"

        echo "=============================================="
        echo "Fixing kubeconfig for DinD environment"
        echo "=============================================="

        # Collect all possible connection endpoints
        echo "=== Collecting connection endpoints ==="

        # 1. Get host-mapped port
        local host_port
        host_port=$(docker port "$control_plane_container" 6443 2>/dev/null | head -1 | cut -d: -f2)
        if [ -z "$host_port" ]; then
          host_port=$(docker inspect "$control_plane_container" --format='{{range $k,$v := .NetworkSettings.Ports}}{{if eq $k "6443/tcp"}}{{(index $v 0).HostPort}}{{end}}{{end}}' 2>/dev/null)
        fi
        echo "Host-mapped port: ${host_port:-NOT FOUND}"

        # 2. Get container IP on kind network
        local container_ip_kind
        container_ip_kind=$(docker inspect -f '{{.NetworkSettings.Networks.kind.IPAddress}}' "$control_plane_container" 2>/dev/null)
        echo "Container IP (kind network): ${container_ip_kind:-NOT FOUND}"

        # 3. Get container IP on bridge network (if kind was created with --network bridge)
        local container_ip_bridge
        container_ip_bridge=$(docker inspect -f '{{.NetworkSettings.Networks.bridge.IPAddress}}' "$control_plane_container" 2>/dev/null)
        echo "Container IP (bridge network): ${container_ip_bridge:-NOT FOUND}"

        # 4. Get first available IP from any network
        local container_ip_any
        container_ip_any=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' "$control_plane_container" 2>/dev/null | head -1)
        echo "Container IP (any network): ${container_ip_any:-NOT FOUND}"

        # 5. Get DinD container IP (the "docker" service)
        local dind_ip
        dind_ip=$(getent hosts docker 2>/dev/null | awk '{print $1}' | head -1)
        echo "DinD service IP ('docker' hostname): ${dind_ip:-NOT FOUND}"

        # 6. Check localhost accessibility
        echo "Localhost check: attempting to reach localhost:${host_port}"

        echo ""
        echo "=== Network topology dump ==="
        echo "All Docker networks:"
        docker network ls 2>&1

        echo ""
        echo "Kind network details:"
        docker network inspect kind 2>&1 | head -80 || echo "Kind network not found"

        echo ""
        echo "Bridge network details:"
        docker network inspect bridge 2>&1 | head -80 || echo "Bridge network not found"

        echo ""
        echo "Control plane container network settings:"
        docker inspect "$control_plane_container" --format='{{json .NetworkSettings.Networks}}' 2>&1 | python3 -m json.tool 2>/dev/null || \
          docker inspect "$control_plane_container" --format='{{json .NetworkSettings.Networks}}' 2>&1 || true

        echo ""
        echo "Control plane container port mappings:"
        docker port "$control_plane_container" 2>&1 || true

        echo ""
        echo "/etc/hosts in this container:"
        cat /etc/hosts 2>&1 || true

        echo ""
        echo "=== Testing all possible connection methods ==="

        # Define all endpoints to try (in order of preference)
        local endpoints=""

        # Method 1: DinD hostname with host-mapped port (GitLab CI standard)
        if [ -n "$host_port" ]; then
          endpoints="$endpoints docker:${host_port}"
        fi

        # Method 2: DinD IP with host-mapped port
        if [ -n "$host_port" ] && [ -n "$dind_ip" ]; then
          endpoints="$endpoints ${dind_ip}:${host_port}"
        fi

        # Method 3: localhost with host-mapped port (if DinD shares network namespace)
        if [ -n "$host_port" ]; then
          endpoints="$endpoints localhost:${host_port}"
          endpoints="$endpoints 127.0.0.1:${host_port}"
        fi

        # Method 4: Container IP with internal port (if on same network)
        if [ -n "$container_ip_kind" ]; then
          endpoints="$endpoints ${container_ip_kind}:6443"
        fi
        if [ -n "$container_ip_bridge" ]; then
          endpoints="$endpoints ${container_ip_bridge}:6443"
        fi
        if [ -n "$container_ip_any" ]; then
          endpoints="$endpoints ${container_ip_any}:6443"
        fi

        # Test each endpoint
        echo ""
        echo "Testing endpoints: $endpoints"
        echo ""

        for endpoint in $endpoints; do
          echo "--- Testing: https://${endpoint}/healthz ---"
          if curl -s -k --connect-timeout 3 --max-time 5 "https://${endpoint}/healthz" 2>&1; then
            echo ""
            echo "SUCCESS! Endpoint ${endpoint} is reachable!"

            # Configure kubectl to use this endpoint
            echo "Configuring kubectl to use https://${endpoint}"
            kubectl config set-cluster "kind-${cluster_name}" \
              --server="https://${endpoint}" \
              --insecure-skip-tls-verify=true

            # Verify kubectl works
            echo "Verifying kubectl connectivity..."
            if kubectl cluster-info --context "kind-${cluster_name}" 2>&1; then
              echo ""
              echo "=============================================="
              echo "API server is accessible at https://${endpoint}"
              echo "=============================================="
              return 0
            else
              echo "kubectl failed despite curl success, trying next endpoint..."
            fi
          else
            echo "FAILED (connection refused or timeout)"
          fi
          echo ""
        done

        echo "=============================================="
        echo "ALL CONNECTION METHODS FAILED"
        echo "=============================================="
        echo ""
        echo "=== Extended diagnostics ==="

        echo ""
        echo "=== Docker container status ==="
        docker ps -a 2>&1

        echo ""
        echo "=== Control plane container full inspect ==="
        docker inspect "$control_plane_container" 2>&1

        echo ""
        echo "=== Control plane container logs (last 150 lines) ==="
        docker logs "$control_plane_container" 2>&1 | tail -150

        echo ""
        echo "=== Can we reach DinD at all? ==="
        echo "Ping docker hostname:"
        ping -c 2 docker 2>&1 || echo "Ping failed"
        echo ""
        echo "Ping DinD IP ($dind_ip):"
        ping -c 2 "$dind_ip" 2>&1 || echo "Ping failed"

        echo ""
        echo "=== netstat/ss from control plane (if available) ==="
        docker exec "$control_plane_container" ss -tlnp 2>&1 || \
          docker exec "$control_plane_container" netstat -tlnp 2>&1 || \
          echo "Could not get listening ports"

        echo ""
        echo "=== API server process in control plane ==="
        docker exec "$control_plane_container" ps aux 2>&1 | grep -E "kube-apiserver|etcd" || true

        echo ""
        echo "=== iptables in DinD (port forwarding rules) ==="
        iptables -t nat -L -n 2>&1 | head -50 || echo "Could not get iptables rules"

        echo ""
        echo "=== Kind cluster info ==="
        kind get clusters 2>&1
        kind get kubeconfig --name "$cluster_name" 2>&1

        echo ""
        echo "=== Kind export logs ==="
        kind export logs "/tmp/kind-logs-${cluster_name}" --name "$cluster_name" 2>&1 || true
        find "/tmp/kind-logs-${cluster_name}" -type f -name "*.log" -exec echo "=== {} ===" \; -exec tail -30 {} \; 2>/dev/null || true

        return 1
      }
  after_script:
    - |
      set -x
      echo "=== Collecting e2e test artifacts ==="

      # Ensure output directory exists
      mkdir -p test/e2e/output

      # Collect cluster info if available
      if command -v kubectl &> /dev/null && kubectl cluster-info &> /dev/null 2>&1; then
        echo "Collecting cluster state..."
        kubectl cluster-info dump --output-directory=test/e2e/output/cluster-dump || true
        kubectl get all -A -o wide > test/e2e/output/all-resources.txt 2>&1 || true
        kubectl get events -A --sort-by='.lastTimestamp' > test/e2e/output/events.txt 2>&1 || true
        kubectl get pods -A -o wide > test/e2e/output/pods.txt 2>&1 || true
        kubectl get nodes -o wide > test/e2e/output/nodes.txt 2>&1 || true

        # Collect operator logs
        for ns in auth-operator-system auth-operator-helm auth-operator-ha; do
          if kubectl get ns "$ns" &> /dev/null; then
            kubectl logs -n "$ns" -l control-plane=controller-manager --tail=1000 > "test/e2e/output/${ns}-controller-logs.txt" 2>&1 || true
            kubectl logs -n "$ns" -l app.kubernetes.io/component=webhook --tail=1000 > "test/e2e/output/${ns}-webhook-logs.txt" 2>&1 || true
            kubectl logs -n "$ns" -l app.kubernetes.io/component=controller --tail=1000 > "test/e2e/output/${ns}-controller-helm-logs.txt" 2>&1 || true
          fi
        done

        # Collect CRD instances
        kubectl get roledefinitions -A -o yaml > test/e2e/output/roledefinitions.yaml 2>&1 || true
        kubectl get binddefinitions -A -o yaml > test/e2e/output/binddefinitions.yaml 2>&1 || true
        kubectl get webhookauthorizers -A -o yaml > test/e2e/output/webhookauthorizers.yaml 2>&1 || true

        # Collect generated RBAC resources
        kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml > test/e2e/output/generated-clusterroles.yaml 2>&1 || true
        kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml > test/e2e/output/generated-clusterrolebindings.yaml 2>&1 || true
        kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml > test/e2e/output/generated-roles.yaml 2>&1 || true
        kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml > test/e2e/output/generated-rolebindings.yaml 2>&1 || true
      fi

      # Create summary report
      echo "# E2E Test Artifacts" > test/e2e/output/README.md
      echo "" >> test/e2e/output/README.md
      echo "Generated at: $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> test/e2e/output/README.md
      echo "Job: ${CI_JOB_NAME}" >> test/e2e/output/README.md
      echo "Pipeline: ${CI_PIPELINE_ID}" >> test/e2e/output/README.md
      echo "" >> test/e2e/output/README.md
      echo "## Files" >> test/e2e/output/README.md
      ls -la test/e2e/output/ >> test/e2e/output/README.md 2>&1 || true

      echo "=== Artifact collection complete ==="
  artifacts:
    when: always
    expire_in: 7 days
    paths:
      - test/e2e/output/
    reports:
      junit: test/e2e/output/junit*.xml

# E2E tests with single-node kind cluster
e2e-single-node:
  extends: .e2e-base
  script:
    - |
      set -exo pipefail
      echo "=== Running e2e tests on single-node cluster ==="

      # Ensure output directory exists for kind logs
      mkdir -p test/e2e/output

      # Create single-node kind cluster
      echo "Creating single-node kind cluster..."
      kind create cluster \
        --name auth-operator-e2e-single \
        --config test/e2e/kind-config-single.yaml \
        --image ${KIND_NODE_IMAGE} \
        --wait 5m

      # Fix kubeconfig for DinD environment
      fix_kubeconfig_for_dind "auth-operator-e2e-single"

      # Verify cluster is ready
      kubectl get nodes -o wide

      # Build and load operator image
      echo "Building operator image..."
      make docker-build IMG=${E2E_IMG}

      echo "Loading image into kind cluster..."
      kind load docker-image ${E2E_IMG} --name auth-operator-e2e-single

      # Run e2e tests
      echo "Running e2e tests..."
      export KIND_CLUSTER=auth-operator-e2e-single
      export IMG=${E2E_IMG}
      export SKIP_CLUSTER_SETUP=false

      # Run all e2e tests except HA tests (which require multi-node)
      go test -tags=e2e ./test/e2e/ -v -ginkgo.v \
        -ginkgo.label-filter='!ha && !leader-election' \
        -ginkgo.junit-report=test/e2e/output/junit-single-node.xml \
        -timeout 45m

      echo "=== Single-node e2e tests complete ==="
  rules:
    - !reference [.rule, is_mr]
    - !reference [.rule, is_default]
  allow_failure: false

# E2E tests with multi-node kind cluster (HA, leader election)
e2e-multi-node:
  extends: .e2e-base
  tags:
    - aws_run_sysbox_l  # Larger runner for multi-node cluster
  script:
    - |
      set -exo pipefail
      echo "=== Running e2e tests on multi-node cluster ==="

      # Ensure output directory exists for kind logs
      mkdir -p test/e2e/output

      # Create multi-node kind cluster with retry
      echo "Creating multi-node kind cluster..."
      create_kind_cluster "auth-operator-e2e-multi" "test/e2e/kind-config-multi.yaml" "12m"

      # Verify cluster is ready
      kubectl cluster-info --context kind-auth-operator-e2e-multi
      kubectl get nodes -o wide

      # Wait for all nodes to be ready
      echo "Waiting for all nodes to be ready..."
      kubectl wait --for=condition=Ready nodes --all --timeout=5m

      # Build and load operator image
      echo "Building operator image..."
      make docker-build IMG=${E2E_IMG}

      echo "Loading image into kind cluster..."
      kind load docker-image ${E2E_IMG} --name auth-operator-e2e-multi

      # Run e2e tests
      echo "Running HA and leader election e2e tests..."
      export KIND_CLUSTER=auth-operator-e2e-multi
      export IMG=${E2E_IMG}
      export SKIP_CLUSTER_SETUP=false

      # Run HA and leader election tests
      go test -tags=e2e ./test/e2e/ -v -ginkgo.v \
        -ginkgo.label-filter='ha || leader-election' \
        -ginkgo.junit-report=test/e2e/output/junit-multi-node.xml \
        -timeout 45m

      echo "=== Multi-node e2e tests complete ==="
  rules:
    - !reference [.rule, is_mr]
    - !reference [.rule, is_default]
  allow_failure: false

# Helm chart e2e tests
e2e-helm:
  extends: .e2e-base
  script:
    - |
      set -exo pipefail
      echo "=== Running Helm e2e tests ==="

      # Ensure output directory exists for kind logs
      mkdir -p test/e2e/output

      # Create kind cluster
      echo "Creating kind cluster for Helm tests..."
      kind create cluster \
        --name auth-operator-e2e-helm \
        --config test/e2e/kind-config-single.yaml \
        --image ${KIND_NODE_IMAGE} \
        --wait 5m

      # Fix kubeconfig for DinD environment
      fix_kubeconfig_for_dind "auth-operator-e2e-helm"

      # Verify cluster is ready
      kubectl get nodes -o wide

      # Build and load operator image
      echo "Building operator image..."
      make docker-build IMG=${E2E_IMG}

      echo "Loading image into kind cluster..."
      kind load docker-image ${E2E_IMG} --name auth-operator-e2e-helm

      # Lint Helm chart first
      echo "Linting Helm chart..."
      helm lint chart/auth-operator --strict

      # Run Helm e2e tests
      echo "Running Helm e2e tests..."
      export KIND_CLUSTER=auth-operator-e2e-helm
      export IMG=${E2E_IMG}
      export SKIP_CLUSTER_SETUP=false

      go test -tags=e2e ./test/e2e/ -v -ginkgo.v \
        -ginkgo.label-filter='helm' \
        -ginkgo.junit-report=test/e2e/output/junit-helm.xml \
        -timeout 45m

      echo "=== Helm e2e tests complete ==="
  rules:
    - !reference [.rule, is_mr]
    - !reference [.rule, is_default]
  allow_failure: false

# Full e2e test suite (runs all tests)
e2e-full:
  extends: .e2e-base
  tags:
    - aws_run_sysbox_xl  # XL runner for comprehensive testing
  script:
    - |
      set -exo pipefail
      echo "=== Running full e2e test suite ==="

      # Ensure output directory exists for kind logs
      mkdir -p test/e2e/output

      # Create multi-node kind cluster for comprehensive testing with retry
      echo "Creating multi-node kind cluster..."
      create_kind_cluster "auth-operator-e2e-full" "test/e2e/kind-config-multi.yaml" "10m"

      # Verify cluster is ready
      kubectl cluster-info --context kind-auth-operator-e2e-full
      kubectl get nodes -o wide

      # Wait for all nodes to be ready
      echo "Waiting for all nodes to be ready..."
      kubectl wait --for=condition=Ready nodes --all --timeout=5m

      # Build and load operator image
      echo "Building operator image..."
      make docker-build IMG=${E2E_IMG}

      echo "Loading image into kind cluster..."
      kind load docker-image ${E2E_IMG} --name auth-operator-e2e-full

      # Run all e2e tests
      echo "Running complete e2e test suite..."
      export KIND_CLUSTER=auth-operator-e2e-full
      export IMG=${E2E_IMG}
      export SKIP_CLUSTER_SETUP=false

      go test -tags=e2e ./test/e2e/ -v -ginkgo.v \
        -ginkgo.junit-report=test/e2e/output/junit-full.xml \
        -timeout 60m

      echo "=== Full e2e test suite complete ==="
  rules:
    - !reference [.rule, is_release]
    - when: manual
      allow_failure: true
  allow_failure: false

# Quick e2e smoke test (for MRs to save time)
e2e-smoke:
  extends: .e2e-base
  tags:
    - aws_run_sysbox_s  # Smallest runner for quick tests
  script:
    - |
      set -exo pipefail
      echo "=== Running e2e smoke tests ==="

      # Ensure output directory exists for kind logs
      mkdir -p test/e2e/output

      # Create minimal kind cluster with single-node config for DinD compatibility
      echo "Creating kind cluster..."
      kind create cluster \
        --name auth-operator-e2e-smoke \
        --config test/e2e/kind-config-single.yaml \
        --image ${KIND_NODE_IMAGE} \
        --wait 3m

      # Fix kubeconfig for DinD environment
      fix_kubeconfig_for_dind "auth-operator-e2e-smoke"

      # Verify cluster is ready
      kubectl get nodes

      # Build and load operator image
      echo "Building operator image..."
      make docker-build IMG=${E2E_IMG}

      echo "Loading image into kind cluster..."
      kind load docker-image ${E2E_IMG} --name auth-operator-e2e-smoke

      # Run quick setup verification tests only
      echo "Running smoke tests..."
      export KIND_CLUSTER=auth-operator-e2e-smoke
      export IMG=${E2E_IMG}
      export SKIP_CLUSTER_SETUP=false

      go test -tags=e2e ./test/e2e/ -v -ginkgo.v \
        -ginkgo.label-filter='setup || debug' \
        -ginkgo.junit-report=test/e2e/output/junit-smoke.xml \
        -timeout 15m

      echo "=== Smoke tests complete ==="
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
      when: manual
      allow_failure: true
  allow_failure: true
