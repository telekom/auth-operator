# SPDX-FileCopyrightText: 2026 Deutsche Telekom AG
#
# SPDX-License-Identifier: Apache-2.0

name: Output Delta

on:
  pull_request:
    branches: [main]
    paths:
      - 'api/**'
      - 'internal/**'
      - 'pkg/**'
      - 'config/samples/**'
      - 'go.mod'
      - 'go.sum'

permissions:
  contents: read
  pull-requests: write

env:
  GO_VERSION: "1.25"
  KIND_VERSION: "v0.31.0"

jobs:
  output-delta:
    name: Generate Output Delta
    runs-on: ubuntu-latest
    steps:
      - name: Checkout PR branch
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          path: pr-branch
          fetch-depth: 0 # Needed to fetch tags

      - name: Checkout main branch
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: main
          path: main-branch

      - name: Get latest release tag
        id: latest-tag
        run: |
          cd pr-branch
          # Get all version tags, sort by semver, and take the latest
          # Filter for tags matching v*.*.* pattern (semver)
          LATEST_TAG=$(git tag -l 'v*.*.*' 2>/dev/null | sort -V | tail -1 || echo "")
          if [ -n "$LATEST_TAG" ]; then
            # Verify the tag actually exists and is reachable
            if git rev-parse "$LATEST_TAG" >/dev/null 2>&1; then
              echo "tag=$LATEST_TAG" >> $GITHUB_OUTPUT
              echo "has_tag=true" >> $GITHUB_OUTPUT
              echo "Latest release tag (by semver): $LATEST_TAG"
            else
              echo "::warning::Tag $LATEST_TAG exists but is not reachable, skipping tag comparison"
              echo "has_tag=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::notice::No release tags found (looking for v*.*.* pattern), skipping tag comparison"
            echo "has_tag=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout latest tag
        if: steps.latest-tag.outputs.has_tag == 'true'
        id: checkout-tag
        continue-on-error: true
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ steps.latest-tag.outputs.tag }}
          path: tag-branch

      - name: Setup Go
        uses: actions/setup-go@0aaccfd150d50ccaeb58ebd88d36e91967a5f35b # v5.4.0
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: pr-branch/go.sum

      - name: Install kind
        run: go install sigs.k8s.io/kind@${{ env.KIND_VERSION }}

      - name: Install yq
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq

      - name: Create kind cluster
        run: |
          kind create cluster --name output-delta --wait 5m
          kubectl cluster-info

      - name: Install popular CRDs
        run: |
          KIND_CLUSTER_NAME=output-delta bash pr-branch/hack/ci/setup-kind-crds.sh

      - name: Create test namespaces
        run: |
          # Create namespaces that match the sample selectors to test Roles and RoleBindings
          # These namespaces exercise different label selector scenarios:
          # - matchLabels (exact match)
          # - matchExpressions with In, Exists operators
          # - Multiple labels for complex selectors

          # Tenant Alpha namespaces (tests matchLabels with tenant + environment)
          kubectl create namespace tenant-alpha
          kubectl label namespace tenant-alpha \
            t-caas.telekom.com/tenant=alpha \
            t-caas.telekom.com/environment=development \
            t-caas.telekom.com/owner=tenant \
            t-caas.telekom.com/purpose=application

          kubectl create namespace tenant-alpha-staging
          kubectl label namespace tenant-alpha-staging \
            t-caas.telekom.com/tenant=alpha \
            t-caas.telekom.com/environment=staging \
            t-caas.telekom.com/owner=tenant

          kubectl create namespace tenant-alpha-prod
          kubectl label namespace tenant-alpha-prod \
            t-caas.telekom.com/tenant=alpha \
            t-caas.telekom.com/environment=production \
            t-caas.telekom.com/owner=tenant

          # Tenant Beta namespaces
          kubectl create namespace tenant-beta
          kubectl label namespace tenant-beta \
            t-caas.telekom.com/tenant=beta \
            t-caas.telekom.com/environment=staging \
            t-caas.telekom.com/owner=tenant

          # Platform namespaces (tests matchLabels with owner=platform)
          kubectl create namespace t-caas-system
          kubectl label namespace t-caas-system \
            t-caas.telekom.com/owner=platform \
            kubernetes.io/metadata.name=t-caas-system

          kubectl create namespace t-caas-monitoring
          kubectl label namespace t-caas-monitoring \
            t-caas.telekom.com/owner=platform \
            t-caas.telekom.com/monitoring=enabled \
            t-caas.telekom.com/component=observability

          kubectl create namespace t-caas-logging
          kubectl label namespace t-caas-logging \
            t-caas.telekom.com/owner=platform \
            kubernetes.io/metadata.name=t-caas-logging

          # GitOps namespaces (tests matchExpressions with Exists operator)
          kubectl create namespace argocd
          kubectl label namespace argocd \
            argocd.argoproj.io/managed-by=argocd \
            t-caas.telekom.com/gitops-source=true

          kubectl create namespace flux-system
          kubectl label namespace flux-system \
            kustomize.toolkit.fluxcd.io/managed-by=flux \
            t-caas.telekom.com/gitops-source=true

          # Shared namespaces (tests matchExpressions with In operator for owner in [tenant, shared])
          kubectl create namespace shared-services
          kubectl label namespace shared-services \
            t-caas.telekom.com/owner=shared \
            t-caas.telekom.com/tenant=shared

          # Compliance namespace (tests specific label for auditor access)
          kubectl create namespace compliance-pci
          kubectl label namespace compliance-pci \
            t-caas.telekom.com/owner=tenant \
            t-caas.telekom.com/tenant=compliance \
            t-caas.telekom.com/compliance-scope=pci-dss

          # CI/CD namespace for tenant alpha (tests matchExpressions with purpose in [cicd, gitops, build])
          kubectl create namespace tenant-alpha-cicd
          kubectl label namespace tenant-alpha-cicd \
            t-caas.telekom.com/tenant=alpha \
            t-caas.telekom.com/purpose=cicd \
            t-caas.telekom.com/owner=tenant

          echo "Test namespaces created:"
          kubectl get namespaces --show-labels | grep -E '(tenant-|t-caas-|argocd|flux-|shared-|compliance-)'

      - name: Generate outputs from main branch
        id: main-output
        continue-on-error: true
        run: |
          cd main-branch

          # Build and load the operator image
          make docker-build IMG=auth-operator:main
          kind load docker-image auth-operator:main --name output-delta

          # Install CRDs
          make install

          # Deploy the operator
          make deploy IMG=auth-operator:main
          echo "Waiting for operator to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=controller-manager -A 2>/dev/null || \
            echo "::warning::Could not verify controller-manager readiness, continuing anyway"

          # Wait for webhook server if deployed (required for BindDefinition validation)
          echo "Waiting for webhook server to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=webhook-server -A 2>/dev/null || \
            echo "::notice::Webhook server not found or not ready, continuing anyway"
          sleep 5

          # Try to apply sample configs with retry - this may fail if main branch samples are incompatible
          main_samples_applied=false
          for i in 1 2 3 4 5; do
            if kubectl apply --server-side -k config/samples/ 2>&1; then
              echo "Waiting for reconciliation..."
              sleep 15
              main_samples_applied=true
              break
            fi
            echo "Attempt $i failed, waiting before retry..."
            sleep 10
          done
          echo "main_samples_applied=${main_samples_applied}" >> $GITHUB_OUTPUT
          if [ "$main_samples_applied" = "false" ]; then
            echo "::warning::Main branch samples failed to apply, will use PR branch samples as baseline"
          fi

          # Wait for all CRs to be fully reconciled (Ready condition)
          echo "Waiting for resources to be reconciled..."
          kubectl wait --for=condition=Ready roledefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some RoleDefinitions not fully reconciled"
          kubectl wait --for=condition=Ready binddefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some BindDefinitions not fully reconciled"

          # Capture generated resources (may be empty if samples failed)
          # Remove internal K8s fields for cleaner diff
          mkdir -p /tmp/main-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/clusterroles.yaml || echo "# No ClusterRoles found" > /tmp/main-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/roles.yaml || echo "# No Roles found" > /tmp/main-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/clusterrolebindings.yaml || echo "# No ClusterRoleBindings found" > /tmp/main-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/rolebindings.yaml || echo "# No RoleBindings found" > /tmp/main-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/serviceaccounts.yaml || echo "# No ServiceAccounts found" > /tmp/main-output/serviceaccounts.yaml

          # Capture CRD status for debugging
          kubectl get roledefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/main-output/roledefinitions-status.yaml || true
          kubectl get binddefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/main-output/binddefinitions-status.yaml || true

          # Cleanup samples - delete CRs and wait for finalizers to be processed before undeploying
          kubectl delete -k config/samples/ --ignore-not-found --timeout=60s 2>/dev/null || true
          # Wait for CRs to be fully deleted (finalizers processed by controller)
          echo "Waiting for CRs to be fully deleted..."
          kubectl wait --for=delete roledefinition --all --timeout=60s 2>/dev/null || true
          kubectl wait --for=delete binddefinition --all --timeout=60s 2>/dev/null || true
          kubectl wait --for=delete webhookauthorizer --all --timeout=60s 2>/dev/null || true
          # Now safe to undeploy the operator
          make undeploy 2>/dev/null || true

          echo "Main branch outputs captured"

      - name: Fallback - use PR samples for main baseline
        if: steps.main-output.outputs.main_samples_applied == 'false' || steps.main-output.outcome == 'failure'
        run: |
          echo "::notice::Using PR branch samples as baseline since main branch samples failed to apply"

          cd pr-branch

          # Ensure operator image is built and loaded into kind for fallback baseline
          make docker-build IMG=auth-operator:pr
          kind load docker-image auth-operator:pr --name output-delta

          # Ensure operator is deployed
          make deploy IMG=auth-operator:pr
          kubectl wait --for=condition=available --timeout=180s deployment -l control-plane=controller-manager -A 2>/dev/null || \
            echo "::warning::Controller-manager deployment not Available in time (fallback baseline will continue anyway)"

          # Wait for webhook server to be ready (required for BindDefinition validation)
          echo "Waiting for webhook server to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=webhook-server -A 2>/dev/null || \
            echo "::warning::Webhook server not ready in time, continuing anyway"
          sleep 5

          # Apply PR samples to generate baseline with retry
          echo "Applying sample configs..."
          for i in 1 2 3 4 5; do
            if kubectl apply --server-side -k config/samples/ 2>&1; then
              echo "Samples applied successfully"
              break
            fi
            echo "Attempt $i failed, waiting before retry..."
            sleep 10
          done

          # Wait for all CRs to be fully reconciled (Ready condition)
          echo "Waiting for resources to be reconciled..."
          kubectl wait --for=condition=Ready roledefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some RoleDefinitions not fully reconciled"
          kubectl wait --for=condition=Ready binddefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some BindDefinitions not fully reconciled"

          # Capture as "main" baseline
          # Remove internal K8s fields for cleaner diff
          mkdir -p /tmp/main-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/clusterroles.yaml || echo "# No ClusterRoles found" > /tmp/main-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/roles.yaml || echo "# No Roles found" > /tmp/main-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/clusterrolebindings.yaml || echo "# No ClusterRoleBindings found" > /tmp/main-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/rolebindings.yaml || echo "# No RoleBindings found" > /tmp/main-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/main-output/serviceaccounts.yaml || echo "# No ServiceAccounts found" > /tmp/main-output/serviceaccounts.yaml

          # Capture CRD status for debugging
          kubectl get roledefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/main-output/roledefinitions-status.yaml || true
          kubectl get binddefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/main-output/binddefinitions-status.yaml || true

          # Cleanup for PR branch fresh run
          kubectl delete -k config/samples/ --ignore-not-found

          echo "Fallback baseline captured using PR branch samples"
          echo "MAIN_FALLBACK=true" >> $GITHUB_ENV

      - name: Generate outputs from latest tag
        id: tag-output
        if: steps.latest-tag.outputs.has_tag == 'true' && steps.checkout-tag.outcome == 'success'
        continue-on-error: true
        run: |
          # Check if tag-branch directory exists (checkout may have failed)
          if [ ! -d "tag-branch" ]; then
            echo "::warning::Tag checkout failed, skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          cd tag-branch

          # Build and load the operator image from tag
          make docker-build IMG=auth-operator:tag 2>/dev/null || {
            echo "::warning::Failed to build tag version (${{ steps.latest-tag.outputs.tag }}), skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          }
          kind load docker-image auth-operator:tag --name output-delta

          # Install CRDs from tag (may differ from main/PR)
          make install 2>/dev/null || true

          # Deploy the operator from tag
          make deploy IMG=auth-operator:tag 2>/dev/null || {
            echo "::warning::Failed to deploy tag version, skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          }
          echo "Waiting for operator to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=controller-manager -A 2>/dev/null || true

          # Wait for webhook server if deployed (required for BindDefinition validation)
          echo "Waiting for webhook server to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=webhook-server -A 2>/dev/null || \
            echo "::notice::Webhook server not found or not ready, continuing anyway"
          sleep 5

          # Try to apply sample configs from tag with retry
          tag_success=false
          for i in 1 2 3 4 5; do
            if kubectl apply --server-side -k config/samples/ 2>&1; then
              echo "Waiting for reconciliation..."
              sleep 15
              tag_success=true
              break
            fi
            echo "Attempt $i failed, waiting before retry..."
            sleep 10
          done
          echo "tag_success=${tag_success}" >> $GITHUB_OUTPUT
          if [ "$tag_success" = "false" ]; then
            echo "::warning::Tag branch samples failed to apply, skipping tag comparison"
            exit 0
          fi

          # Wait for all CRs to be fully reconciled (Ready condition)
          echo "Waiting for resources to be reconciled..."
          kubectl wait --for=condition=Ready roledefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some RoleDefinitions not fully reconciled"
          kubectl wait --for=condition=Ready binddefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some BindDefinitions not fully reconciled"

          # Capture generated resources
          # Remove internal K8s fields for cleaner diff
          mkdir -p /tmp/tag-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/tag-output/clusterroles.yaml || echo "# No ClusterRoles found" > /tmp/tag-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/tag-output/roles.yaml || echo "# No Roles found" > /tmp/tag-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/tag-output/clusterrolebindings.yaml || echo "# No ClusterRoleBindings found" > /tmp/tag-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/tag-output/rolebindings.yaml || echo "# No RoleBindings found" > /tmp/tag-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/tag-output/serviceaccounts.yaml || echo "# No ServiceAccounts found" > /tmp/tag-output/serviceaccounts.yaml

          # Capture CRD status for debugging
          kubectl get roledefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/tag-output/roledefinitions-status.yaml || true
          kubectl get binddefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/tag-output/binddefinitions-status.yaml || true

          # Cleanup samples - delete CRs and wait for finalizers to be processed before undeploying
          kubectl delete -k config/samples/ --ignore-not-found --timeout=60s 2>/dev/null || true
          # Wait for CRs to be fully deleted (finalizers processed by controller)
          echo "Waiting for CRs to be fully deleted..."
          kubectl wait --for=delete roledefinition --all --timeout=60s 2>/dev/null || true
          kubectl wait --for=delete binddefinition --all --timeout=60s 2>/dev/null || true
          kubectl wait --for=delete webhookauthorizer --all --timeout=60s 2>/dev/null || true
          # Now safe to undeploy the operator
          make undeploy 2>/dev/null || true

          echo "Tag branch (${{ steps.latest-tag.outputs.tag }}) outputs captured"

      - name: Generate outputs from PR branch
        id: pr-output
        run: |
          cd pr-branch

          # Build and load the operator image
          make docker-build IMG=auth-operator:pr
          kind load docker-image auth-operator:pr --name output-delta

          # Update CRDs if changed
          make install

          # Deploy the operator from PR branch
          make deploy IMG=auth-operator:pr
          echo "Waiting for operator to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=controller-manager -A 2>/dev/null || \
            echo "::warning::Could not verify controller-manager readiness, continuing anyway"

          # Wait for webhook server to be ready (required for BindDefinition validation)
          echo "Waiting for webhook server to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=webhook-server -A 2>/dev/null || \
            echo "::warning::Could not verify webhook-server readiness, continuing anyway"
          sleep 5

          # Apply sample configs with retry (webhook may need additional time to be fully ready)
          echo "Applying sample configs..."
          for i in 1 2 3 4 5; do
            if kubectl apply --server-side -k config/samples/ 2>&1; then
              echo "Samples applied successfully"
              break
            fi
            echo "Attempt $i failed, waiting before retry..."
            sleep 10
          done

          # Wait for all CRs to be fully reconciled (Ready condition)
          echo "Waiting for resources to be reconciled..."
          kubectl wait --for=condition=Ready roledefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some RoleDefinitions not fully reconciled"
          kubectl wait --for=condition=Ready binddefinition --all --timeout=120s 2>/dev/null || echo "::notice::Some BindDefinitions not fully reconciled"

          # Capture generated resources
          # Remove internal K8s fields for cleaner diff
          mkdir -p /tmp/pr-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/pr-output/clusterroles.yaml || echo "# No ClusterRoles found" > /tmp/pr-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/pr-output/roles.yaml || echo "# No Roles found" > /tmp/pr-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/pr-output/clusterrolebindings.yaml || echo "# No ClusterRoleBindings found" > /tmp/pr-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/pr-output/rolebindings.yaml || echo "# No RoleBindings found" > /tmp/pr-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.managedFields, .items[].metadata.ownerReferences[].uid, .metadata)' > /tmp/pr-output/serviceaccounts.yaml || echo "# No ServiceAccounts found" > /tmp/pr-output/serviceaccounts.yaml

          # Capture CRD status for debugging
          kubectl get roledefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/pr-output/roledefinitions-status.yaml || true
          kubectl get binddefinitions -o yaml 2>/dev/null | yq 'del(.items[].metadata.creationTimestamp, .items[].metadata.managedFields, .items[].metadata.resourceVersion, .items[].metadata.uid, .items[].metadata.generation, .items[].metadata.ownerReferences[].uid, .items[].status.observedGeneration, .items[].status.conditions[].lastTransitionTime, .metadata)' > /tmp/pr-output/binddefinitions-status.yaml || true

          echo "PR branch outputs captured"

      - name: Collect controller logs
        if: always()
        run: |
          mkdir -p /tmp/controller-logs

          # Get auth-operator controller manager pod logs from all namespaces
          echo "Collecting controller logs..."

          # Try to find the controller manager pod in common namespaces
          for ns in auth-operator-system kube-system default; do
            kubectl logs -n $ns -l control-plane=controller-manager --tail=5000 > /tmp/controller-logs/${ns}-controller.log 2>/dev/null || true
            kubectl logs -n $ns -l app.kubernetes.io/name=auth-operator --tail=5000 >> /tmp/controller-logs/${ns}-controller.log 2>/dev/null || true
          done

          # Get webhook server logs
          for ns in auth-operator-system kube-system default; do
            kubectl logs -n $ns -l control-plane=webhook-server --tail=5000 > /tmp/controller-logs/${ns}-webhook.log 2>/dev/null || true
          done

          # Get all events
          kubectl get events -A --sort-by='.lastTimestamp' > /tmp/controller-logs/all-events.txt 2>/dev/null || true

          # Get pod statuses
          kubectl get pods -A > /tmp/controller-logs/all-pods.txt 2>/dev/null || true

          # Get CRD statuses
          kubectl get roledefinitions -A -o yaml > /tmp/controller-logs/roledefinitions.yaml 2>/dev/null || true
          kubectl get binddefinitions -A -o yaml > /tmp/controller-logs/binddefinitions.yaml 2>/dev/null || true

          echo "Controller logs collected"

      - name: Extract errors from logs
        if: always()
        id: extract-errors
        run: |
          mkdir -p /tmp/error-summary
          
          echo "Extracting errors and failures from logs..."
          
          # Extract error lines from controller logs (case-insensitive)
          # Match: ERROR, error, Error, FAIL, fail, Fail, panic, PANIC
          ERROR_LOG="/tmp/error-summary/errors.txt"
          echo "# Error Summary from Controller Logs" > "$ERROR_LOG"
          echo "" >> "$ERROR_LOG"
          
          for log in /tmp/controller-logs/*.log; do
            if [ -f "$log" ] && [ -s "$log" ]; then
              log_name=$(basename "$log")
              errors=$(grep -iE '(\"level\":\"error|ERROR|\serror\s|\[error\]|level=error|panic:|PANIC|failed|failure|Failed|Failure)' "$log" 2>/dev/null | head -50 || true)
              if [ -n "$errors" ]; then
                echo "## $log_name" >> "$ERROR_LOG"
                echo '```' >> "$ERROR_LOG"
                echo "$errors" >> "$ERROR_LOG"
                echo '```' >> "$ERROR_LOG"
                echo "" >> "$ERROR_LOG"
              fi
            fi
          done
          
          # Extract warning events
          echo "## Warning/Error Events" >> "$ERROR_LOG"
          echo '```' >> "$ERROR_LOG"
          grep -iE '(Warning|Error|Failed|BackOff)' /tmp/controller-logs/all-events.txt 2>/dev/null | tail -30 >> "$ERROR_LOG" || echo "No warning events found" >> "$ERROR_LOG"
          echo '```' >> "$ERROR_LOG"
          echo "" >> "$ERROR_LOG"
          
          # Extract CRD conditions with False status
          echo "## CRD Conditions (non-Ready)" >> "$ERROR_LOG"
          echo '```yaml' >> "$ERROR_LOG"
          for crd in roledefinitions binddefinitions; do
            if [ -f "/tmp/controller-logs/${crd}.yaml" ]; then
              yq '.items[] | select(.status.conditions != null) | select(.status.conditions[] | select(.status != "True")) | {"name": .metadata.name, "conditions": .status.conditions}' "/tmp/controller-logs/${crd}.yaml" 2>/dev/null >> "$ERROR_LOG" || true
            fi
          done
          echo '```' >> "$ERROR_LOG"
          
          # Check if we found any errors
          error_count=$(grep -cE '(ERROR|error|panic|failed|failure|Warning)' "$ERROR_LOG" 2>/dev/null || echo "0")
          if [ "$error_count" -gt 5 ]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
            echo "Found $error_count error indicators in logs"
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
            echo "No significant errors found in logs"
          fi
          
          # Store error content for PR comment (truncate if too long)
          ERROR_CONTENT=$(cat "$ERROR_LOG" | head -100)
          echo "errors<<ERROREOF" >> $GITHUB_OUTPUT
          echo "$ERROR_CONTENT" >> $GITHUB_OUTPUT
          echo "ERROREOF" >> $GITHUB_OUTPUT
          
          echo "Error extraction complete"

      - name: Upload controller logs
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: controller-logs
          path: /tmp/controller-logs/
          retention-days: 7

      - name: Upload error summary
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: error-summary
          path: /tmp/error-summary/
          retention-days: 7

      - name: Upload output artifacts
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: output-delta-artifacts
          path: |
            /tmp/main-output/
            /tmp/pr-output/
            /tmp/tag-output/
          retention-days: 7

      - name: Generate diff
        id: diff
        run: |
          mkdir -p /tmp/diff-output

          # Add fallback notice if applicable
          if [ "$MAIN_FALLBACK" = "true" ]; then
            echo "‚ö†Ô∏è NOTE: Main branch samples failed to apply. Using PR branch samples as baseline." >> /tmp/diff-output/full-diff.txt
            echo "This means no meaningful diff can be generated - showing PR output only." >> /tmp/diff-output/full-diff.txt
            echo "" >> /tmp/diff-output/full-diff.txt
            echo "main_fallback=true" >> $GITHUB_OUTPUT
          else
            echo "main_fallback=false" >> $GITHUB_OUTPUT
          fi

          # Generate separate diff files for each resource type (main vs PR)
          for resource in clusterroles roles clusterrolebindings rolebindings serviceaccounts; do
            diff -u /tmp/main-output/$resource.yaml /tmp/pr-output/$resource.yaml > /tmp/diff-output/$resource.diff 2>&1 || true
          done

          # Include CRD status diffs in separate files
          for resource in roledefinitions-status binddefinitions-status; do
            if [ -f "/tmp/main-output/$resource.yaml" ] && [ -f "/tmp/pr-output/$resource.yaml" ]; then
              diff -u /tmp/main-output/$resource.yaml /tmp/pr-output/$resource.yaml > /tmp/diff-output/$resource.diff 2>&1 || true
            fi
          done

          # Generate tag diffs if tag outputs exist (append to resource files with header)
          if [ -d "/tmp/tag-output" ] && [ "${{ steps.tag-output.outputs.tag_success }}" = "true" ]; then
            for resource in clusterroles roles clusterrolebindings rolebindings serviceaccounts; do
              if [ -s "/tmp/diff-output/$resource.diff" ]; then
                echo "" >> /tmp/diff-output/$resource.diff
                echo "# Changes from latest release (${{ steps.latest-tag.outputs.tag }})" >> /tmp/diff-output/$resource.diff
              fi
              diff -u /tmp/tag-output/$resource.yaml /tmp/pr-output/$resource.yaml >> /tmp/diff-output/$resource.diff 2>&1 || true
            done
            echo "has_tag_diff=true" >> $GITHUB_OUTPUT
          else
            echo "has_tag_diff=false" >> $GITHUB_OUTPUT
          fi

          # Also create combined diff for backward compatibility
          cat /tmp/diff-output/*.diff > /tmp/diff-output/full-diff.txt 2>/dev/null || true

          # Check if there are any differences
          # When MAIN_FALLBACK=true, main and PR outputs are from the same branch,
          # so force has_diff=true to avoid misleading "no changes" comment
          if [ "$MAIN_FALLBACK" = "true" ]; then
            echo "has_diff=true" >> $GITHUB_OUTPUT
            echo "Forcing has_diff=true due to main fallback - comparison not meaningful"
          elif diff -q /tmp/main-output /tmp/pr-output > /dev/null 2>&1; then
            echo "has_diff=false" >> $GITHUB_OUTPUT
            echo "No differences found between main and PR branch outputs"
          else
            echo "has_diff=true" >> $GITHUB_OUTPUT
            echo "Differences found!"
          fi

          # Store diff for comment (truncate if too long)
          DIFF_CONTENT=$(cat /tmp/diff-output/full-diff.txt | head -500)

          # Use a delimiter for multiline output
          echo "diff<<EOF" >> $GITHUB_OUTPUT
          echo "$DIFF_CONTENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload diff artifacts
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: diff-output
          path: /tmp/diff-output/
          retention-days: 7

      - name: Comment PR with delta
        if: steps.diff.outputs.has_diff == 'true' || steps.extract-errors.outputs.has_errors == 'true'
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const fs = require('fs');
            
            // Helper to read diff file and check if it has meaningful content
            function readDiff(filename) {
              try {
                const content = fs.readFileSync('/tmp/diff-output/' + filename, 'utf8');
                // Check if diff has actual changes (not just empty or header-only)
                if (content.trim() && content.includes('@@')) {
                  return content.substring(0, 10000);
                }
              } catch (e) {
                // File doesn't exist or can't be read
              }
              return '';
            }
            
            // Read per-resource diffs
            const resourceDiffs = {
              'ClusterRoles': readDiff('clusterroles.diff'),
              'Roles': readDiff('roles.diff'),
              'ClusterRoleBindings': readDiff('clusterrolebindings.diff'),
              'RoleBindings': readDiff('rolebindings.diff'),
              'ServiceAccounts': readDiff('serviceaccounts.diff'),
              'RoleDefinitions Status': readDiff('roledefinitions-status.diff'),
              'BindDefinitions Status': readDiff('binddefinitions-status.diff')
            };
            
            let errorSummary = '';
            try {
              errorSummary = fs.readFileSync('/tmp/error-summary/errors.txt', 'utf8').substring(0, 10000);
            } catch (e) {
              console.log('Could not read error summary file:', e.message);
            }
            
            const mainFallback = '${{ steps.diff.outputs.main_fallback }}' === 'true';
            const hasErrors = '${{ steps.extract-errors.outputs.has_errors }}' === 'true';

            let fallbackNotice = '';
            if (mainFallback) {
              fallbackNotice = '\n> ‚ö†Ô∏è **Note:** Main branch samples failed to apply (likely due to schema changes).\n' +
                '> PR branch samples were used as baseline, so no meaningful diff is shown.\n' +
                '> This typically happens when sample files are updated with new required fields.\n\n';
            }

            const hasTagDiff = '${{ steps.diff.outputs.has_tag_diff }}' === 'true';
            const latestTag = '${{ steps.latest-tag.outputs.tag }}';

            let tagInfo = '';
            if (hasTagDiff && latestTag) {
              tagInfo = '\n\nüì¶ **Also compared against latest release:** `' + latestTag + '`';
            }

            // Build resource sections - one fold-out per resource type with changes
            let resourceSections = '';
            for (const [name, diff] of Object.entries(resourceDiffs)) {
              if (diff) {
                resourceSections += '<details>\n' +
                  '<summary>üì¶ ' + name + '</summary>\n\n' +
                  '```diff\n' + diff + '\n```\n\n' +
                  '</details>\n\n';
              }
            }
            
            if (!resourceSections) {
              resourceSections = '> No resource changes detected.\n\n';
            }

            let errorSection = '';
            if (hasErrors && errorSummary) {
              errorSection = '---\n\n' +
                '## ‚ö†Ô∏è Controller Logs\n\n' +
                '<details>\n' +
                '<summary>Errors/Warnings Found in Logs (click to expand)</summary>\n\n' +
                errorSummary + '\n\n' +
                '</details>\n';
            }

            // Build body with separate fold-outs per resource type
            const body = '## üìä Output Delta Report\n' +
              fallbackNotice +
              'This PR changes the generated RBAC resources when applied to sample configurations.' + tagInfo + '\n\n' +
              resourceSections +
              errorSection;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Output Delta Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Comment PR - no changes
        if: steps.diff.outputs.has_diff == 'false' && steps.diff.outputs.main_fallback != 'true'
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const body = `## üìä Output Delta Report

            ‚úÖ No changes detected in generated RBAC resources.

            The sample configurations in \`config/samples/\` produce identical output on both main and this PR.
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Output Delta Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Cleanup
        if: always()
        run: kind delete cluster --name output-delta
