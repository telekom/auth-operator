# SPDX-FileCopyrightText: 2026 Deutsche Telekom AG
#
# SPDX-License-Identifier: Apache-2.0

name: Output Delta

on:
  pull_request:
    branches: [main]
    paths:
      - 'api/**'
      - 'internal/**'
      - 'pkg/**'
      - 'config/samples/**'
      - 'go.mod'
      - 'go.sum'

permissions:
  contents: read
  pull-requests: write

env:
  GO_VERSION: "1.25"
  KIND_VERSION: "v0.31.0"

jobs:
  output-delta:
    name: Generate Output Delta
    runs-on: ubuntu-latest
    steps:
      - name: Checkout PR branch
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          path: pr-branch
          fetch-depth: 0 # Needed to fetch tags

      - name: Checkout main branch
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: main
          path: main-branch

      - name: Get latest release tag
        id: latest-tag
        run: |
          cd pr-branch
          # Get all version tags, sort by semver, and take the latest
          # Filter for tags matching v*.*.* pattern (semver)
          LATEST_TAG=$(git tag -l 'v*.*.*' 2>/dev/null | sort -V | tail -1 || echo "")
          if [ -n "$LATEST_TAG" ]; then
            # Verify the tag actually exists and is reachable
            if git rev-parse "$LATEST_TAG" >/dev/null 2>&1; then
              echo "tag=$LATEST_TAG" >> $GITHUB_OUTPUT
              echo "has_tag=true" >> $GITHUB_OUTPUT
              echo "Latest release tag (by semver): $LATEST_TAG"
            else
              echo "::warning::Tag $LATEST_TAG exists but is not reachable, skipping tag comparison"
              echo "has_tag=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::notice::No release tags found (looking for v*.*.* pattern), skipping tag comparison"
            echo "has_tag=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout latest tag
        if: steps.latest-tag.outputs.has_tag == 'true'
        id: checkout-tag
        continue-on-error: true
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: ${{ steps.latest-tag.outputs.tag }}
          path: tag-branch

      - name: Setup Go
        uses: actions/setup-go@0aaccfd150d50ccaeb58ebd88d36e91967a5f35b # v5.4.0
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true
          cache-dependency-path: pr-branch/go.sum

      - name: Install kind
        run: go install sigs.k8s.io/kind@${{ env.KIND_VERSION }}

      - name: Create kind cluster
        run: |
          kind create cluster --name output-delta --wait 5m
          kubectl cluster-info

      - name: Install popular CRDs
        run: |
          KIND_CLUSTER_NAME=output-delta bash pr-branch/hack/ci/setup-kind-crds.sh

      - name: Generate outputs from main branch
        id: main-output
        continue-on-error: true
        run: |
          cd main-branch

          # Build and load the operator image
          make docker-build IMG=auth-operator:main
          kind load docker-image auth-operator:main --name output-delta

          # Install CRDs
          make install

          # Deploy the operator
          make deploy IMG=auth-operator:main
          echo "Waiting for operator to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment/auth-operator-controller-manager -n auth-operator-system 2>/dev/null || \
            kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=controller-manager -n kube-system 2>/dev/null || \
            echo "::warning::Could not verify controller-manager readiness, continuing anyway"

          # Wait for webhook server if deployed
          kubectl wait --for=condition=available --timeout=60s deployment -l control-plane=webhook-server -n auth-operator-system 2>/dev/null || \
            echo "::notice::Webhook server not found or not ready, continuing anyway"

          # Try to apply sample configs - this may fail if main branch samples are incompatible
          if kubectl apply --server-side -k config/samples/ 2>/dev/null; then
            echo "Waiting for reconciliation..."
            sleep 15
            echo "main_samples_applied=true" >> $GITHUB_OUTPUT
          else
            echo "::warning::Main branch samples failed to apply, will use PR branch samples as baseline"
            echo "main_samples_applied=false" >> $GITHUB_OUTPUT
          fi

          # Capture generated resources (may be empty if samples failed)
          mkdir -p /tmp/main-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/clusterroles.yaml 2>/dev/null || echo "# No ClusterRoles found" > /tmp/main-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/roles.yaml 2>/dev/null || echo "# No Roles found" > /tmp/main-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/clusterrolebindings.yaml 2>/dev/null || echo "# No ClusterRoleBindings found" > /tmp/main-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/rolebindings.yaml 2>/dev/null || echo "# No RoleBindings found" > /tmp/main-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/serviceaccounts.yaml 2>/dev/null || echo "# No ServiceAccounts found" > /tmp/main-output/serviceaccounts.yaml

          # Also capture by owner reference pattern (fallback)
          kubectl get clusterroles -o yaml | grep -A 1000 "t-caas" > /tmp/main-output/clusterroles-tcaas.yaml 2>/dev/null || true

          # Cleanup samples and operator for next branch
          kubectl delete -k config/samples/ --ignore-not-found 2>/dev/null || true
          make undeploy 2>/dev/null || true

          echo "Main branch outputs captured"

      - name: Fallback - use PR samples for main baseline
        if: steps.main-output.outputs.main_samples_applied == 'false' || steps.main-output.outcome == 'failure'
        run: |
          echo "::notice::Using PR branch samples as baseline since main branch samples failed to apply"

          cd pr-branch

          # Ensure operator image is built and loaded into kind for fallback baseline
          make docker-build IMG=auth-operator:pr
          kind load docker-image auth-operator:pr --name output-delta

          # Ensure operator is deployed
          make deploy IMG=auth-operator:pr
          kubectl wait --for=condition=available --timeout=60s deployment -l control-plane=controller-manager -A 2>/dev/null
          kubectl wait --for=condition=available --timeout=60s deployment -l control-plane=webhook-server -A 2>/dev/null || true
          sleep 5

          # Apply PR samples to generate baseline
          kubectl apply --server-side -k config/samples/
          echo "Waiting for reconciliation..."
          sleep 15

          # Capture as "main" baseline
          mkdir -p /tmp/main-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/clusterroles.yaml 2>/dev/null || echo "# No ClusterRoles found" > /tmp/main-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/roles.yaml 2>/dev/null || echo "# No Roles found" > /tmp/main-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/clusterrolebindings.yaml 2>/dev/null || echo "# No ClusterRoleBindings found" > /tmp/main-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/rolebindings.yaml 2>/dev/null || echo "# No RoleBindings found" > /tmp/main-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/main-output/serviceaccounts.yaml 2>/dev/null || echo "# No ServiceAccounts found" > /tmp/main-output/serviceaccounts.yaml

          # Cleanup for PR branch fresh run
          kubectl delete -k config/samples/ --ignore-not-found

          echo "Fallback baseline captured using PR branch samples"
          echo "MAIN_FALLBACK=true" >> $GITHUB_ENV

      - name: Generate outputs from latest tag
        id: tag-output
        if: steps.latest-tag.outputs.has_tag == 'true' && steps.checkout-tag.outcome == 'success'
        continue-on-error: true
        run: |
          # Check if tag-branch directory exists (checkout may have failed)
          if [ ! -d "tag-branch" ]; then
            echo "::warning::Tag checkout failed, skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          cd tag-branch

          # Build and load the operator image from tag
          make docker-build IMG=auth-operator:tag 2>/dev/null || {
            echo "::warning::Failed to build tag version (${{ steps.latest-tag.outputs.tag }}), skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          }
          kind load docker-image auth-operator:tag --name output-delta

          # Install CRDs from tag (may differ from main/PR)
          make install 2>/dev/null || true

          # Deploy the operator from tag
          make deploy IMG=auth-operator:tag 2>/dev/null || {
            echo "::warning::Failed to deploy tag version, skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          }
          echo "Waiting for operator to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=controller-manager -A 2>/dev/null || true
          sleep 5

          # Try to apply sample configs from tag
          if kubectl apply --server-side -k config/samples/ 2>/dev/null; then
            echo "Waiting for reconciliation..."
            sleep 15
            echo "tag_success=true" >> $GITHUB_OUTPUT
          else
            echo "::warning::Tag branch samples failed to apply, skipping tag comparison"
            echo "tag_success=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Capture generated resources
          mkdir -p /tmp/tag-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/tag-output/clusterroles.yaml 2>/dev/null || echo "# No ClusterRoles found" > /tmp/tag-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/tag-output/roles.yaml 2>/dev/null || echo "# No Roles found" > /tmp/tag-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/tag-output/clusterrolebindings.yaml 2>/dev/null || echo "# No ClusterRoleBindings found" > /tmp/tag-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/tag-output/rolebindings.yaml 2>/dev/null || echo "# No RoleBindings found" > /tmp/tag-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/tag-output/serviceaccounts.yaml 2>/dev/null || echo "# No ServiceAccounts found" > /tmp/tag-output/serviceaccounts.yaml

          # Cleanup samples and operator for PR branch
          kubectl delete -k config/samples/ --ignore-not-found 2>/dev/null || true
          make undeploy 2>/dev/null || true

          echo "Tag branch (${{ steps.latest-tag.outputs.tag }}) outputs captured"

      - name: Generate outputs from PR branch
        id: pr-output
        run: |
          cd pr-branch

          # Build and load the operator image
          make docker-build IMG=auth-operator:pr
          kind load docker-image auth-operator:pr --name output-delta

          # Update CRDs if changed
          make install

          # Deploy the operator from PR branch
          make deploy IMG=auth-operator:pr
          echo "Waiting for operator to be ready..."
          kubectl wait --for=condition=available --timeout=120s deployment -l control-plane=controller-manager -A 2>/dev/null || \
            echo "::warning::Could not verify operator readiness, continuing anyway"
          sleep 5

          # Apply sample configs and wait for reconciliation
          kubectl apply --server-side -k config/samples/
          echo "Waiting for reconciliation..."
          sleep 15

          # Capture generated resources
          mkdir -p /tmp/pr-output
          kubectl get clusterroles -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/pr-output/clusterroles.yaml 2>/dev/null || echo "# No ClusterRoles found" > /tmp/pr-output/clusterroles.yaml
          kubectl get roles -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/pr-output/roles.yaml 2>/dev/null || echo "# No Roles found" > /tmp/pr-output/roles.yaml
          kubectl get clusterrolebindings -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/pr-output/clusterrolebindings.yaml 2>/dev/null || echo "# No ClusterRoleBindings found" > /tmp/pr-output/clusterrolebindings.yaml
          kubectl get rolebindings -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/pr-output/rolebindings.yaml 2>/dev/null || echo "# No RoleBindings found" > /tmp/pr-output/rolebindings.yaml
          kubectl get serviceaccounts -A -l app.kubernetes.io/created-by=auth-operator -o yaml > /tmp/pr-output/serviceaccounts.yaml 2>/dev/null || echo "# No ServiceAccounts found" > /tmp/pr-output/serviceaccounts.yaml

          # Also capture by owner reference pattern (fallback)
          kubectl get clusterroles -o yaml | grep -A 1000 "t-caas" > /tmp/pr-output/clusterroles-tcaas.yaml 2>/dev/null || true

          echo "PR branch outputs captured"

      - name: Collect controller logs
        if: always()
        run: |
          mkdir -p /tmp/controller-logs

          # Get auth-operator controller manager pod logs from all namespaces
          echo "Collecting controller logs..."

          # Try to find the controller manager pod in common namespaces
          for ns in auth-operator-system kube-system default; do
            kubectl logs -n $ns -l control-plane=controller-manager --tail=5000 > /tmp/controller-logs/${ns}-controller.log 2>/dev/null || true
            kubectl logs -n $ns -l app.kubernetes.io/name=auth-operator --tail=5000 >> /tmp/controller-logs/${ns}-controller.log 2>/dev/null || true
          done

          # Get all pods in kube-system that might be relevant
          kubectl logs -n kube-system -l app=auth-operator --tail=5000 > /tmp/controller-logs/kube-system-auth-operator.log 2>/dev/null || true

          # Get all events
          kubectl get events -A --sort-by='.lastTimestamp' > /tmp/controller-logs/all-events.txt 2>/dev/null || true

          # Get pod statuses
          kubectl get pods -A > /tmp/controller-logs/all-pods.txt 2>/dev/null || true

          # Get CRD statuses
          kubectl get roledefinitions -A -o yaml > /tmp/controller-logs/roledefinitions.yaml 2>/dev/null || true
          kubectl get binddefinitions -A -o yaml > /tmp/controller-logs/binddefinitions.yaml 2>/dev/null || true

          echo "Controller logs collected"

      - name: Upload controller logs
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: controller-logs
          path: /tmp/controller-logs/
          retention-days: 7

      - name: Upload output artifacts
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: output-delta-artifacts
          path: |
            /tmp/main-output/
            /tmp/pr-output/
            /tmp/tag-output/
          retention-days: 7

      - name: Generate diff
        id: diff
        run: |
          mkdir -p /tmp/diff-output

          # Add fallback notice if applicable
          if [ "$MAIN_FALLBACK" = "true" ]; then
            echo "âš ï¸ NOTE: Main branch samples failed to apply. Using PR branch samples as baseline." >> /tmp/diff-output/full-diff.txt
            echo "This means no meaningful diff can be generated - showing PR output only." >> /tmp/diff-output/full-diff.txt
            echo "" >> /tmp/diff-output/full-diff.txt
            echo "main_fallback=true" >> $GITHUB_OUTPUT
          else
            echo "main_fallback=false" >> $GITHUB_OUTPUT
          fi

          # Generate diffs for each resource type (main vs PR)
          echo "## Changes from main branch" >> /tmp/diff-output/full-diff.txt
          echo "" >> /tmp/diff-output/full-diff.txt
          for resource in clusterroles roles clusterrolebindings rolebindings serviceaccounts; do
            echo "=== $resource ===" >> /tmp/diff-output/full-diff.txt
            diff -u /tmp/main-output/$resource.yaml /tmp/pr-output/$resource.yaml >> /tmp/diff-output/full-diff.txt 2>&1 || true
            echo "" >> /tmp/diff-output/full-diff.txt
          done

          # Generate tag diff if tag outputs exist
          if [ -d "/tmp/tag-output" ] && [ "${{ steps.tag-output.outputs.tag_success }}" = "true" ]; then
            echo "" >> /tmp/diff-output/full-diff.txt
            echo "## Changes from latest release (${{ steps.latest-tag.outputs.tag }})" >> /tmp/diff-output/full-diff.txt
            echo "" >> /tmp/diff-output/full-diff.txt
            for resource in clusterroles roles clusterrolebindings rolebindings serviceaccounts; do
              echo "=== $resource ===" >> /tmp/diff-output/full-diff.txt
              diff -u /tmp/tag-output/$resource.yaml /tmp/pr-output/$resource.yaml >> /tmp/diff-output/full-diff.txt 2>&1 || true
              echo "" >> /tmp/diff-output/full-diff.txt
            done
            echo "has_tag_diff=true" >> $GITHUB_OUTPUT
          else
            echo "has_tag_diff=false" >> $GITHUB_OUTPUT
          fi

          # Check if there are any differences
          # When MAIN_FALLBACK=true, main and PR outputs are from the same branch,
          # so force has_diff=true to avoid misleading "no changes" comment
          if [ "$MAIN_FALLBACK" = "true" ]; then
            echo "has_diff=true" >> $GITHUB_OUTPUT
            echo "Forcing has_diff=true due to main fallback - comparison not meaningful"
          elif diff -q /tmp/main-output /tmp/pr-output > /dev/null 2>&1; then
            echo "has_diff=false" >> $GITHUB_OUTPUT
            echo "No differences found between main and PR branch outputs"
          else
            echo "has_diff=true" >> $GITHUB_OUTPUT
            echo "Differences found!"
          fi

          # Store diff for comment (truncate if too long)
          DIFF_CONTENT=$(cat /tmp/diff-output/full-diff.txt | head -500)

          # Use a delimiter for multiline output
          echo "diff<<EOF" >> $GITHUB_OUTPUT
          echo "$DIFF_CONTENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload diff artifacts
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: diff-output
          path: /tmp/diff-output/
          retention-days: 7

      - name: Comment PR with delta
        if: steps.diff.outputs.has_diff == 'true'
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const diff = `${{ steps.diff.outputs.diff }}`;
            const mainFallback = '${{ steps.diff.outputs.main_fallback }}' === 'true';

            let fallbackNotice = '';
            if (mainFallback) {
              fallbackNotice = `
            > âš ï¸ **Note:** Main branch samples failed to apply (likely due to schema changes).
            > PR branch samples were used as baseline, so no meaningful diff is shown.
            > This typically happens when sample files are updated with new required fields.

            `;
            }

            const hasTagDiff = '${{ steps.diff.outputs.has_tag_diff }}' === 'true';
            const latestTag = '${{ steps.latest-tag.outputs.tag }}';

            let tagInfo = '';
            if (hasTagDiff && latestTag) {
              tagInfo = `\n\nðŸ“¦ **Also compared against latest release:** \`${latestTag}\``;
            }

            const body = `## ðŸ“Š Output Delta Report
            ${fallbackNotice}
            This PR changes the generated RBAC resources when applied to sample configurations.${tagInfo}

            <details>
            <summary>Click to expand diff</summary>

            \`\`\`diff
            ${diff}
            \`\`\`

            </details>

            > **Note:** This diff shows changes in ClusterRoles, Roles, ClusterRoleBindings, RoleBindings, and ServiceAccounts generated from \`config/samples/\`.
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Output Delta Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Comment PR - no changes
        if: steps.diff.outputs.has_diff == 'false' && steps.diff.outputs.main_fallback != 'true'
        uses: actions/github-script@ed597411d8f924073f98dfc5c65a23a2325f34cd # v8.0.0
        with:
          script: |
            const body = `## ðŸ“Š Output Delta Report

            âœ… No changes detected in generated RBAC resources.

            The sample configurations in \`config/samples/\` produce identical output on both main and this PR.
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Output Delta Report')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Cleanup
        if: always()
        run: kind delete cluster --name output-delta
