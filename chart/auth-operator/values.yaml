image:
  repository: ghcr.io/telekom/auth-operator
  # Image reference precedence: digest > tag > Chart.AppVersion
  # - digest: immutable reference, preferred for production (e.g., sha256:<digest>)
  # - tag: mutable reference, useful for development
  # - If neither is set, defaults to Chart.AppVersion
  tag: ""
  digest: ""
  pullPolicy: IfNotPresent
imagePullSecrets: []

# Global settings applied to all components
global:
  # Log verbosity level (0-9, higher is more verbose)
  logLevel: 2

serviceAccount:
  annotations: {}

# Node selection constraints (applied to all pods)
nodeSelector: {}

# Tolerations (applied to all pods)
tolerations: []

# Pod affinity rules (applied to all pods)
affinity: {}

# Pod topology spread constraints (applied to all pods)
topologySpreadConstraints: []

# Priority class name for pods
priorityClassName: ""

# Additional pod annotations (applied to all pods)
podAnnotations: {}

# Additional pod labels (applied to all pods)
podLabels: {}

controller:
  # Number of concurrent reconcilers for BindDefinition controller
  bindDefinitionConcurrency: 10
  # Number of concurrent reconcilers for RoleDefinition controller
  roleDefinitionConcurrency: 10
  resources:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 64Mi
  replicas: 1
  podDisruptionBudget:
    # Enable PodDisruptionBudget for high availability
    enabled: false
    # Minimum number of pods that must be available during disruption
    minAvailable: 1
    # Maximum number of pods that can be unavailable during disruption
    # (mutually exclusive with minAvailable)
    # maxUnavailable: 1

webhookServer:
  replicas: 1
  tdgMigration: "false"
  podDisruptionBudget:
    # Enable PodDisruptionBudget for high availability
    enabled: false
    # Minimum number of pods that must be available during disruption
    minAvailable: 1
    # Maximum number of pods that can be unavailable during disruption
    # (mutually exclusive with minAvailable)
    # maxUnavailable: 1
  resources:
    limits:
      cpu: 150m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
  service:
    port: 443
    protocol: TCP
    type: ClusterIP

metrics:
  service:
    # Enable a dedicated Service for Prometheus scraping
    enabled: true
    port: 8080
  serviceMonitor:
    # Enable ServiceMonitor for Prometheus Operator
    enabled: false
    # Scrape interval (uses Prometheus default if empty)
    interval: ""
    # Scrape timeout (uses Prometheus default if empty)
    scrapeTimeout: ""
    # Additional labels for ServiceMonitor (e.g., for label-based selection)
    additionalLabels: {}

# Pod-level security context (applied to all pods)
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65532
  runAsGroup: 65532
  fsGroup: 65532
  seccompProfile:
    type: RuntimeDefault

# Container-level security context (applied to all containers)
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# OpenTelemetry tracing configuration
tracing:
  # Enable distributed tracing via OpenTelemetry
  enabled: false
  # OTLP collector endpoint (e.g. "otel-collector:4317")
  endpoint: ""
  # Trace sampling rate (0.0 to 1.0); 0.1 = 10% of traces
  samplingRate: 0.1
  # Use insecure (non-TLS) connection to the collector
  insecure: true
