image:
  repository: ghcr.io/telekom/auth-operator
  # Image reference precedence: digest > tag > Chart.AppVersion
  # - digest: immutable reference, preferred for production (e.g., sha256:<digest>)
  # - tag: mutable reference, useful for development
  # - If neither is set, defaults to Chart.AppVersion
  tag: ""
  digest: ""
  pullPolicy: IfNotPresent
imagePullSecrets: []

# Global settings applied to all components
global:
  # Log verbosity level (0-9, higher is more verbose)
  logLevel: 2

serviceAccount:
  annotations: {}

# Node selection constraints (applied to all pods)
nodeSelector: {}

# Tolerations (applied to all pods)
tolerations: []

# Pod affinity rules (applied to all pods)
affinity: {}

# Pod topology spread constraints (applied to all pods)
topologySpreadConstraints: []

# Priority class name for pods
priorityClassName: ""

# Additional pod annotations (applied to all pods)
podAnnotations: {}

# Additional pod labels (applied to all pods)
podLabels: {}

controller:
  # Number of concurrent reconcilers for BindDefinition controller
  bindDefinitionConcurrency: 10
  # Number of concurrent reconcilers for RoleDefinition controller
  roleDefinitionConcurrency: 10
  resources:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 64Mi
  replicas: 1
  podDisruptionBudget:
    # Enable PodDisruptionBudget for high availability
    enabled: false
    # Minimum number of pods that must be available during disruption
    minAvailable: 1
    # Maximum number of pods that can be unavailable during disruption
    # (mutually exclusive with minAvailable)
    # maxUnavailable: 1

webhookServer:
  replicas: 1
  tdgMigration: "false"
  podDisruptionBudget:
    # Enable PodDisruptionBudget for high availability
    enabled: false
    # Minimum number of pods that must be available during disruption
    minAvailable: 1
    # Maximum number of pods that can be unavailable during disruption
    # (mutually exclusive with minAvailable)
    # maxUnavailable: 1
  resources:
    limits:
      cpu: 150m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
  service:
    port: 443
    protocol: TCP
    type: ClusterIP

metrics:
  service:
    # Enable a dedicated Service for Prometheus scraping
    enabled: true
    port: 8080
  serviceMonitor:
    # Enable ServiceMonitor for Prometheus Operator
    enabled: false
    # Scrape interval (uses Prometheus default if empty)
    interval: ""
    # Scrape timeout (uses Prometheus default if empty)
    scrapeTimeout: ""
    # Additional labels for ServiceMonitor (e.g., for label-based selection)
    additionalLabels: {}

# Pod-level security context (applied to all pods)
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65532
  runAsGroup: 65532
  fsGroup: 65532
  seccompProfile:
    type: RuntimeDefault

# Container-level security context (applied to all containers)
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# Network policies for restricting pod-level ingress
networkPolicy:
  # Enable NetworkPolicy resources for webhook and metrics port isolation
  enabled: false
  # Namespace that hosts the monitoring stack (Prometheus)
  # Used to restrict metrics port (8080) access on the controller-manager
  metricsNamespace: monitoring
  webhookServer:
    # Custom ingress `from` rules for the webhook port (9443).
    # By default, allows all namespaces (required for kube-apiserver on host network).
    # Override to restrict further if your CNI supports host-network policies.
    # Example:
    #   ingressFrom:
    #     - ipBlock:
    #         cidr: 10.0.0.0/8
    ingressFrom: []
  controllerManager:
    # Custom ingress `from` rules for the metrics port (8080).
    # By default, restricts to metricsNamespace only.
    # Override to allow additional namespaces.
    ingressFrom: []
  # Egress rules â€” required when a default-deny egress policy exists in the namespace.
  # When enabled, allows the operator pods to reach DNS, the Kubernetes API server,
  # and optionally webhook callback endpoints.
  egress:
    enabled: false
    # dnsNamespace is the namespace where CoreDNS runs (for UDP/TCP 53).
    dnsNamespace: kube-system
    # apiServerCIDR restricts egress to the API server by IP range.
    # Leave empty to allow egress to any destination on ports 443/6443 (less restrictive).
    # Example: "10.96.0.1/32" for the default Kubernetes service IP.
    apiServerCIDR: ""
    # additionalRules allows injecting custom egress rules (e.g., webhook callbacks).
    # Each entry follows the standard NetworkPolicy egress rule schema.
    # Example:
    #   additionalRules:
    #     - ports:
    #         - port: 443
    #           protocol: TCP
    #       to:
    #         - namespaceSelector:
    #             matchLabels:
    #               kubernetes.io/metadata.name: cert-manager
    additionalRules: []
